
#1. Installation and Importing Libraries                  
         
!pip install transformers
import transformers
from transformers import BertTokenizer,BertConfig,BertModel
import torch
import pickle
from torch import nn
import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, auc
from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader
   		
 #2. Dataset preparation to train the model

class InputExample(object):
    def __init__(self, id, text, labels=None):
        self.id = id
        self.text = text
        self.labels = labels

class InputFeatures(object):
    def __init__(self, input_ids, input_mask, segment_ids, label_ids):
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.segment_ids = segment_ids
        self.label_ids = label_ids

def get_train_examples(train_file):
    # train_df = pd.read_csv(train_file)
    train_df = train_file
    # size = 800      # sample size
    # replace = False  # with replacement
    # fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]
    # train_df = train_df.groupby('label_num', as_index=False).apply(fn)
    # train_df = train_df.sample(frac = 1)
    ids = train_df["Unnamed: 0"].values
    text = train_df['news'].values
    # labels = train_df[train_df.columns[5:]].values
    labels = train_df[train_df.columns[11:]].values
    examples = []
    for i in range(len(train_df)):
        examples.append(InputExample(ids[i], text[i], labels=labels[i]))
    return examples

def get_features_from_examples(examples, max_seq_len, tokenizer):
    features = []
    for i,example in enumerate(examples):
        tokens = tokenizer.tokenize(example.text)
        if len(tokens) > max_seq_len - 2:
            tokens = tokens[:(max_seq_len - 2)]
        tokens = ["[CLS]"] + tokens + ["[SEP]"]
        input_ids = tokenizer.convert_tokens_to_ids(tokens)
        input_mask = [1] * len(input_ids)
        segment_ids = [0] * len(tokens)
        padding = [0] * (max_seq_len - len(input_ids))
        input_ids += padding
        input_mask += padding
        segment_ids += padding
        assert len(input_ids) == max_seq_len
        assert len(input_mask) == max_seq_len
        assert len(segment_ids) == max_seq_len
        label_ids = [float(label) for label in example.labels]
        features.append(InputFeatures(input_ids=input_ids,
                                      input_mask=input_mask,
                                      segment_ids=segment_ids,
                                      label_ids=label_ids))
    return features
def get_dataset_from_features(features):
    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
    input_mask = torch.tensor([f.input_mask for f in features],
 dtype= torch.long)
    segment_ids = torch.tensor([f.segment_ids for f in features], dtype=
    torch.long)
    label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.float)
    dataset = TensorDataset(input_ids,
                            input_mask,
                            segment_ids,
                            label_ids)
    return dataset
    
#3 .CNN Model

class CNN(nn.Module):
    def __init__(self, embed_num, embed_dim, dropout=0.5, kernel_num=3,
                      kernel_sizes=[2,3,4], num_labels=2):
        super().__init__()
        self.num_labels = num_labels
        self.embed_num = embed_num
        self.embed_dim = embed_dim
        self.dropout = dropout
        self.kernel_num = kernel_num
        self.kernel_sizes = kernel_sizes
        self.embed = nn.Embedding(self.embed_num, self.embed_dim)
        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, 
        self.embed_dim)) for k in self.kernel_sizes])
        self.dropout = nn.Dropout(self.dropout)
        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, 
        self.num_labels)
         
    def forward(self, inputs, labels=None):
        output = inputs.unsqueeze(1)
        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in 
        self.convs]
        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in 
        output]
        output = torch.cat(output, 1)
        output = self.dropout(output)
        logits = self.classifier(output)
        if labels is not None:
            loss_fct = nn.BCEWithLogitsLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, 
            self.num_labels))
            return loss
        else:
            return logits


# 4. Initializing  BERT Tokenzier and BERT Embeddings

device = torch.device(type='cuda')
pretrained_weights = 'bert-base-cased'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)
config = BertConfig.from_pretrained(pretrained_weights, 
output_hidden_states=True)
basemodel = BertModel.from_pretrained(pretrained_weights,
config=config)
basemodel.to(device)

#.5 Loading DataSet and Extracting Features From News

seq_len = 128
# train_file = "/content/drive/MyDrive/FakeNewsDataSets/fakeNewsLatest.csv"
train_file = df
train_examples = get_train_examples(train_file)
train_features = get_features_from_examples(train_examples, seq_len, 
tokenizer)
train_dataset = get_dataset_from_features(train_features)
train_val_split = 0.2
train_size = int(len(train_dataset)*(1-train_val_split))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset,
 [train_size, val_size])
batch = 8
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, 
batch_size=batch)
val_sampler = SequentialSampler(val_dataset)
val_dataloader = DataLoader(val_dataset, sampler=val_sampler, 
batch_size=batch)



#6. Initializing The CNN Model

embed_num = seq_len 
embed_dim = basemodel.config.hidden_size 
dropout = basemodel.config.hidden_dropout_prob
kernel_num = 3
kernel_sizes = [2,3,4]
num_labels = 6

model = KimCNN(embed_num, embed_dim, dropout=dropout, kernel_num=kernel_num, kernel_sizes=kernel_sizes, num_labels=num_labels)
model.to(device)

# 7 .Training The Model

lr = 3e-5
epochs = 20
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
labels = ['barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire ', 'true']
# labels = ["False"]
for i in range(epochs):
    print('-----------EPOCH #{}-----------'.format(i+1))
    print('training...')
    model.train()
    for step, batch in enumerate(train_dataloader):
        batch = tuple(t.to(device) for t in batch)
        input_ids, input_mask, segment_ids, label_ids = batch
        with torch.no_grad():
            inputs = basemodel(input_ids, segment_ids, input_mask)
            inputs = inputs[0]
        loss = model(inputs, label_ids)
        loss = loss.mean()
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# 8. Evaluating The Model

y_true = []
y_pred = []
  model.eval()
  print('evaluating...')
  for step, batch in enumerate(val_dataloader):
      batch = tuple(t.to(device) for t in batch)
      val_input_ids, val_input_mask, val_segment_ids, val_label_ids = batch
      with torch.no_grad():
       val_inputs = basemodel(val_input_ids, val_segment_ids, val_input_mask)
        val_inputs = val_inputs[0]
         logits = model(val_inputs)
         y_true.append(val_label_ids)
         y_pred.append(logits)
         y_true = torch.cat(y_true, dim=0).float().cpu().detach().numpy()
         y_pred = torch.cat(y_pred, dim=0).float().cpu().detach().numpy()

    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i,label in enumerate(labels):
        fpr[label], tpr[label], _ = roc_curve(y_true[:, i], y_pred[:, i])
        roc_auc[label] = auc(fpr[label], tpr[label])

    print('ROC AUC per label:')
    for label in labels:
        print(label, ': ', roc_auc[label])




# 9.Printing The Evaluated Outputs

def sigmoid(z):
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    return s

for i in np.random.randint(0, len(y_pred), size=10):
    string = tokenizer.decode(val_dataset[i][0], skip_special_tokens=True)
    print('---------------------------------')
    print('news:')
    print(string)
    preds = dict(zip(labels, sigmoid(y_pred[i])))
    print('Prediction:')
    for label in preds:
        print(label, ': ', preds[label])
